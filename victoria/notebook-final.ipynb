{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d91764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "TRAIN_PATH = \"../train.csv\"\n",
    "EVAL_PATH = \"../eval.csv\"\n",
    "SUBMISSION = \"submission.csv\"\n",
    "MODEL_PATH = \"model.pkl\"\n",
    "\n",
    "CHAR_NGRAMS = (2, 6)\n",
    "MIN_DF = 6\n",
    "MAX_DF = 0.60\n",
    "MAXFEAT_CHAR = 300_000\n",
    "STRIP_ACCENTS = None\n",
    "\n",
    "LR_C = 2.3\n",
    "LR_PENALTY = \"l2\"\n",
    "LR_CLASS_WEIGHT = \"balanced\"\n",
    "LR_MAX_ITER = 12000\n",
    "RANDOM_SEED = 42\n",
    "HOLDOUT_FRAC = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26549f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_WORD_EXPERIMENT = False\n",
    "\n",
    "if ENABLE_WORD_EXPERIMENT:\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "        nltk.download(\"wordnet\", quiet=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import stanza\n",
    "        try:\n",
    "            stanza.download(\"es\", processors=\"tokenize,pos,lemma\", verbose=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "        nlp_es = stanza.Pipeline(\"es\", processors=\"tokenize,pos,lemma\",\n",
    "                                 tokenize_no_ssplit=False, use_gpu=False, verbose=False)\n",
    "    except Exception:\n",
    "        nlp_es = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67929a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _normalize_basic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text)).lower()\n",
    "    text = re.sub(r\"-\\s*\\n\", \"\", text)\n",
    "    text = re.sub(r\"\\s*\\n\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def _clean_text_list(text_list):\n",
    "    cleaned = []\n",
    "    for item in text_list:\n",
    "        cleaned.append(_normalize_basic(item))\n",
    "    return cleaned\n",
    "\n",
    "def _force_decade_3digits(value):\n",
    "    s = str(abs(int(value)))\n",
    "    if len(s) >= 3:\n",
    "        return int(s[:3])\n",
    "    return int(s)\n",
    "\n",
    "def _read_train(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for chunk in pd.read_csv(path, chunksize=8000, encoding=\"utf-8-sig\",\n",
    "                             engine=\"python\", on_bad_lines=\"skip\", dtype={\"text\": str, \"decade\": str}):\n",
    "        if \"text\" in chunk and \"decade\" in chunk:\n",
    "            subset = chunk[[\"text\", \"decade\"]].dropna()\n",
    "            for t in subset[\"text\"].tolist():\n",
    "                texts.append(str(t))\n",
    "            for y in subset[\"decade\"].tolist():\n",
    "                labels.append(_force_decade_3digits(int(y)))\n",
    "    if len(texts) == 0:\n",
    "        raise ValueError(\"Train vacío tras carga.\")\n",
    "    df = pd.DataFrame({\"text\": texts, \"decade\": labels})\n",
    "    return df[[\"text\", \"decade\"]]\n",
    "\n",
    "def _stream_eval(path, needed_text_key=\"text\"):\n",
    "    ids = []\n",
    "    texts = []\n",
    "    header = pd.read_csv(path, nrows=0, encoding=\"utf-8-sig\", engine=\"python\", on_bad_lines=\"skip\").columns.tolist()\n",
    "    if \"id\" not in header:\n",
    "        raise ValueError(f\"eval.csv debe tener 'id'. Columnas: {header}\")\n",
    "    if needed_text_key in header:\n",
    "        key = needed_text_key\n",
    "    else:\n",
    "        if \"text\" in header:\n",
    "            key = \"text\"\n",
    "        else:\n",
    "            if \"texto\" in header:\n",
    "                key = \"texto\"\n",
    "            else:\n",
    "                key = None\n",
    "    if not key:\n",
    "        raise ValueError(f\"eval.csv debe tener '{needed_text_key}' o 'text'/'texto'. Columnas: {header}\")\n",
    "    for chunk in pd.read_csv(path, chunksize=8000, encoding=\"utf-8-sig\",\n",
    "                             engine=\"python\", on_bad_lines=\"skip\", dtype=str):\n",
    "        if \"id\" in chunk and key in chunk:\n",
    "            subset = chunk[[\"id\", key]].dropna()\n",
    "            for v in subset[\"id\"].tolist():\n",
    "                ids.append(v)\n",
    "            for v in subset[key].astype(str).tolist():\n",
    "                texts.append(v)\n",
    "    return ids, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = _read_train(TRAIN_PATH)\n",
    "texts_all = _clean_text_list(df_train[\"text\"].tolist())\n",
    "labels_all = df_train[\"decade\"].to_numpy()\n",
    "n_samples = len(labels_all)\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "unique_labels = np.unique(labels_all)\n",
    "test_mask = np.zeros(n_samples, dtype=bool)\n",
    "for label in unique_labels:\n",
    "    idx_label = np.where(labels_all == label)[0]\n",
    "    rng.shuffle(idx_label)\n",
    "    n_test_label = max(1, int(round(HOLDOUT_FRAC * len(idx_label))))\n",
    "    for i in range(n_test_label):\n",
    "        test_mask[idx_label[i]] = True\n",
    "\n",
    "train_mask = ~test_mask\n",
    "train_indices = np.where(train_mask)[0]\n",
    "test_indices = np.where(test_mask)[0]\n",
    "\n",
    "texts_train = []\n",
    "for i in train_indices:\n",
    "    texts_train.append(texts_all[i])\n",
    "labels_train = labels_all[train_mask]\n",
    "\n",
    "texts_test = []\n",
    "for i in test_indices:\n",
    "    texts_test.append(texts_all[i])\n",
    "labels_test = labels_all[test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HOLDOUT] acc=0.2840 (CHAR; C=2.3, min_df=6, max_df=0.6, maxfeat=300000, strip=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=CHAR_NGRAMS,\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF,\n",
    "    sublinear_tf=True,\n",
    "    dtype=np.float32,\n",
    "    max_features=MAXFEAT_CHAR,\n",
    "    strip_accents=STRIP_ACCENTS\n",
    ")\n",
    "X_train = vectorizer.fit_transform(texts_train)\n",
    "X_test = vectorizer.transform(texts_test)\n",
    "\n",
    "model = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    penalty=LR_PENALTY,\n",
    "    C=LR_C,\n",
    "    class_weight=LR_CLASS_WEIGHT,\n",
    "    max_iter=LR_MAX_ITER,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "model.fit(X_train, labels_train)\n",
    "\n",
    "acc_holdout = float((model.predict(X_test) == labels_test).mean())\n",
    "print(f\"[HOLDOUT] acc={acc_holdout:.4f} (CHAR; C={LR_C}, min_df={MIN_DF}, max_df={MAX_DF}, maxfeat={MAXFEAT_CHAR}, strip=None)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1aac6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_all = vectorizer.fit_transform(texts_all)\n",
    "model.fit(X_all, labels_all)\n",
    "dump({\"vectorizer\": vectorizer, \"model\": model}, MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae30eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish submission: submission.csv (3490 filas)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    eval_ids, eval_texts = _stream_eval(EVAL_PATH, needed_text_key=\"text\")\n",
    "    eval_texts_clean = _clean_text_list(eval_texts)\n",
    "    X_eval = vectorizer.transform(eval_texts_clean)\n",
    "    eval_preds = model.predict(X_eval)\n",
    "    eval_preds_3 = []\n",
    "    for p in eval_preds:\n",
    "        eval_preds_3.append(_force_decade_3digits(p))\n",
    "    pd.DataFrame({\"id\": eval_ids, \"decade\": eval_preds_3}).to_csv(SUBMISSION, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Finish submission: {SUBMISSION} ({len(eval_ids)} filas)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No se encontró eval.csv; no se generó submission.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
